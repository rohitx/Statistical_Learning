2.4.3: We now revisit the bias-variance decomposition.

(a) Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.

**Answer**

- To describe in words, the variance increases with increasing flexibility
- Bias decreases with increasing flexibility
- Training error decreases with increasing flexibility
- Test Error first decreases and then increasing with increasing flexibility
- The Bayes error remains constant.

The test error in general makes a U-shaped curve with increasing flexibility.

(b) Explain why each of the five curves has the shape displayed in part (a)

**Answer**
- Variance refers to the amount by which f would change if we estimated it using a different training data set. Generally variance does not change much between data set. However, when the model is highly flexible, it fits as many points as possible. If just one point is off in a given training data set, the change will be considerably different from the rest of the training set.
- Bias decreases with increasing flexibility because there is less restriction on how the model should be. In other words, we add more degrees of freedom as flexibility increases. The model gets less rigid and is therefore able to fit better.
- The training error decreases with increasing flexibility because increasing the degrees of freedom allows the model to mimic the data closely.
- The test error has a U-shaped form because as the flexibility increases, the model is able to predict accurately. However, there comes a point when the flexibility of model overfits the data. This causes the model to predict or look for patterns that do not really exist. Thus the test error increases.
- Bayes (irreducible) error - defines the lower limit, the test error is bounded below by the irreducible error due to variance in the error (epsilon) in the output values (0 <= value). When the training error is lower than the irreducible error,overfitting has taken place. The Bayes error rate is defined for classification problems and is determined by the ratio of data points which lie at the 'wrong' side of the decision boundary, (0 <= value < 1).
